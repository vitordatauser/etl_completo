# Pipeline ETL com Airflow - AWS

Este repositÃ³rio contÃ©m o pipeline de dados ETL (Extract, Transform, Load) utilizando Apache Airflow na AWS.

## ğŸ“ Estrutura do Projeto

```
etl_completo/
â”œâ”€â”€ dags/                    # DAGs do Airflow
â”‚   â””â”€â”€ extract_data.py      # Exemplo de DAG
â”œâ”€â”€ plugins/                 # Plugins customizados do Airflow
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ include/                 # CÃ³digo compartilhado e reutilizÃ¡vel
â”‚   â”œâ”€â”€ utils/              # FunÃ§Ãµes utilitÃ¡rias
â”‚   â”œâ”€â”€ operators/           # Operadores customizados
â”‚   â”œâ”€â”€ hooks/               # Hooks customizados (conexÃµes AWS, DB, etc)
â”‚   â””â”€â”€ sensors/            # Sensors customizados
â”œâ”€â”€ scripts/                 # Scripts auxiliares (deploy, setup, etc)
â”œâ”€â”€ sql/                     # Queries SQL reutilizÃ¡veis
â”‚   â”œâ”€â”€ raw/                 # Queries para dados brutos
â”‚   â””â”€â”€ transformed/         # Queries para dados transformados
â”œâ”€â”€ tests/                   # Testes automatizados
â”‚   â”œâ”€â”€ unit/                # Testes unitÃ¡rios
â”‚   â””â”€â”€ integration/         # Testes de integraÃ§Ã£o
â”œâ”€â”€ config/                  # Arquivos de configuraÃ§Ã£o
â”œâ”€â”€ data/                    # Dados de exemplo/teste (nÃ£o versionar dados reais)
â”‚   â”œâ”€â”€ raw/                 # Dados brutos de exemplo
â”‚   â””â”€â”€ processed/           # Dados processados de exemplo
â”œâ”€â”€ logs/                    # Logs locais (gitignored)
â””â”€â”€ .github/workflows/       # CI/CD workflows

```

## ğŸ“‚ DescriÃ§Ã£o das Pastas

### `dags/`
ContÃ©m todas as DAGs do Airflow. Cada arquivo Python representa uma ou mais DAGs que definem o fluxo de trabalho do pipeline.

### `plugins/`
Plugins customizados do Airflow que estendem a funcionalidade padrÃ£o (operadores, hooks, sensors, executors, etc).

### `include/`
CÃ³digo Python compartilhado entre DAGs:
- **utils/**: FunÃ§Ãµes utilitÃ¡rias (validaÃ§Ã£o, formataÃ§Ã£o, logging, etc)
- **operators/**: Operadores customizados reutilizÃ¡veis
- **hooks/**: Hooks para conexÃµes com serviÃ§os AWS (S3, Redshift, RDS, etc)
- **sensors/**: Sensors customizados para aguardar condiÃ§Ãµes especÃ­ficas

### `scripts/`
Scripts auxiliares para:
- Deploy para AWS (MWAA - Managed Workflows for Apache Airflow)
- Setup do ambiente local
- MigraÃ§Ãµes de banco de dados
- UtilitÃ¡rios de linha de comando

### `sql/`
Queries SQL organizadas por tipo:
- **raw/**: Queries para extraÃ§Ã£o de dados brutos
- **transformed/**: Queries para transformaÃ§Ã£o e agregaÃ§Ã£o

### `tests/`
Testes automatizados:
- **unit/**: Testes unitÃ¡rios de funÃ§Ãµes e classes
- **integration/**: Testes de integraÃ§Ã£o com serviÃ§os AWS

### `config/`
Arquivos de configuraÃ§Ã£o (YAML, JSON, etc) para diferentes ambientes (dev, staging, prod).

### `data/`
Dados de exemplo para desenvolvimento e testes locais. **NÃ£o versionar dados sensÃ­veis ou grandes volumes**.

### `logs/`
Logs gerados localmente (adicionado ao .gitignore).

## ğŸš€ Como Usar

### Setup Inicial

1. **Configure o ambiente virtual** (recomendado):
```bash
./scripts/setup_env.sh
```

Ou manualmente:
```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
pip install -r requirements.txt
```

2. **Ative o ambiente virtual** (sempre antes de trabalhar):
```bash
source venv/bin/activate
```

3. **Configure as variÃ¡veis de ambiente** (copie `.env.example` para `.env`)

4. **Execute o Airflow localmente** ou faÃ§a deploy para AWS MWAA

ğŸ“– **Para mais detalhes, consulte o guia completo:** [SETUP.md](SETUP.md)

## ğŸ“¤ Conectando ao GitHub

Para conectar este repositÃ³rio ao GitHub e fazer push das alteraÃ§Ãµes, consulte o guia completo em [GITHUB_SETUP.md](GITHUB_SETUP.md).

**Resumo rÃ¡pido:**
```bash
git init
git add .
git commit -m "Initial commit"
git remote add origin https://github.com/vitordatauser/etl_completo.git
git branch -M main
git push -u origin main
```

## ğŸ“ Notas

- Mantenha as DAGs simples e delegue lÃ³gica complexa para mÃ³dulos em `include/`
- Use `plugins/` para funcionalidades especÃ­ficas do Airflow
- Organize SQL por tipo de operaÃ§Ã£o (raw vs transformed)
- Escreva testes para cÃ³digo crÃ­tico em `tests/`
